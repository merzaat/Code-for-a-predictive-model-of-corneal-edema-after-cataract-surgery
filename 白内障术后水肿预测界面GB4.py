from fastapi import FastAPI, HTTPException, UploadFile, File, Body
from fastapi.middleware.cors import CORSMiddleware
import joblib
import pandas as pd
import numpy as np
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    roc_auc_score, roc_curve, accuracy_score,
    precision_score, recall_score, f1_score, confusion_matrix
)
from imblearn.over_sampling import SMOTE
import å…¨éƒ¨
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
from matplotlib.patches import Patch
from statsmodels.nonparametric.smoothers_lowess import lowess
from sklearn.ensemble import GradientBoostingClassifier
import warnings
import base64
from io import BytesIO

warnings.filterwarnings('ignore')

# åˆå§‹åŒ–FastAPI
app = FastAPI(title="Cataract Postoperative Corneal Damage Prediction System", version="1.0")

# è·¨åŸŸé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ ¸å¿ƒé…ç½®
EXCEL_PATH = "CEML3ç‰¹å¾äº¤é›†.xlsx"
MODEL_PATH = "corneal_damage_model.pkl"
SCALER_PATH = "data_scaler.pkl"
SHAP_EXPLAINER_PATH = "shap_explainer.pkl"
ROC_SAVE_PATH = "roc_curve.png"
SHAP_BEESWARM_PATH = "shap_beeswarm.png"  # SHAPèœ‚ç¾¤å›¾ä¿å­˜è·¯å¾„
SHAP_SCATTER_PATH = "shap_scatter.png"    # SHAPæ•£ç‚¹æ‹Ÿåˆå›¾ä¿å­˜è·¯å¾„
TARGET_COL = "è§’è†œæŸä¼¤çŠ¶æ€"
DATA_SAVE_EXCEL_PATH = "dataset_split.xlsx"
TOP_N_FEATURES = 15  # æ•£ç‚¹æ‹Ÿåˆå›¾å±•ç¤ºçš„TOPç‰¹å¾æ•°

# å…¨å±€å˜é‡
model = None
scaler = None
shap_explainer = None
FEATURE_NAMES = []
shap_beeswarm_base64 = None  # å­˜å‚¨SHAPèœ‚ç¾¤å›¾çš„base64ç¼–ç 
shap_scatter_base64 = None   # å­˜å‚¨SHAPæ•£ç‚¹æ‹Ÿåˆå›¾çš„base64ç¼–ç 
global_shap_values = None    # å…¨é‡æ ·æœ¬çš„SHAPå€¼ï¼ˆå…¨å±€ï¼‰
global_feature_values = None # å…¨é‡æ ·æœ¬çš„åŸå§‹ç‰¹å¾å€¼ï¼ˆå…¨å±€ï¼‰
global_y_values = None       # å…¨é‡æ ·æœ¬çš„ç›®æ ‡å€¼ï¼ˆç”¨äºç»„åˆå›¾åˆ†ç±»ç»Ÿè®¡ï¼‰

# ä¸´åºŠå»ºè®®ç”Ÿæˆå‡½æ•°
def generate_clinical_advice(risk_level, risk_prob, input_params):
    """æ ¹æ®é£é™©ç­‰çº§ã€æ¦‚ç‡å’Œè¾“å…¥å‚æ•°ç”Ÿæˆä¸´åºŠå»ºè®®"""
    age = input_params.get("å¹´é¾„", 0)
    anterior_chamber_depth = input_params.get("å‰æˆ¿æ·±åº¦", 0)
    total_surgery_time = input_params.get("æ€»æ‰‹æœ¯æ—¶é—´", 0)
    negative_pressure_time = input_params.get("è´Ÿå‹æ—¶é—´", 0)
    effective_emulsification_time = input_params.get("æœ‰æ•ˆä¹³åŒ–æ—¶é—´", 0)
    
    if risk_level == "ä½é£é™©":
        return f"""ã€æœ¯å‰å»ºè®®ã€‘
1. è§’è†œæŸä¼¤é£é™©{risk_prob}%ï¼ˆä½é£é™©ï¼‰ï¼Œå¸¸è§„è¡Œè§’è†œå†…çš®ç»†èƒè®¡æ•°æ£€æŸ¥ï¼›
2. å¹´é¾„{age}å²ï¼Œè¯„ä¼°åŸºç¡€ç–¾ç—…æ§åˆ¶æƒ…å†µï¼›
3. å‰æˆ¿æ·±åº¦{anterior_chamber_depth}mmï¼Œæ‰‹æœ¯æŒ‰å¸¸è§„æµç¨‹è¿›è¡Œã€‚

ã€æœ¯ä¸­å»ºè®®ã€‘
1. æ€»æ‰‹æœ¯æ—¶é—´æ§åˆ¶åœ¨{total_surgery_time}ç§’å†…ï¼›
2. è´Ÿå‹æ—¶é—´ç»´æŒå®‰å…¨èŒƒå›´ã€‚

ã€æœ¯åå»ºè®®ã€‘
1. æœ¯å1ã€3å¤©å¤æŸ¥è§’è†œæ°´è‚¿ï¼›
2. å±€éƒ¨ç”¨æŠ—ç”Ÿç´ +æ¿€ç´ æ»´çœ¼æ¶²1å‘¨ï¼›
3. 1ä¸ªæœˆåéšè®¿è§†åŠ›æ¢å¤ã€‚"""
    
    elif risk_level == "ä¸­é£é™©":
        return f"""ã€æœ¯å‰å»ºè®®ã€‘
1. è§’è†œæŸä¼¤é£é™©{risk_prob}%ï¼ˆä¸­é£é™©ï¼‰ï¼Œå®Œå–„è§’è†œå†…çš®ç»†èƒå¯†åº¦æ£€æµ‹ï¼›
2. å¹´é¾„{age}å²ï¼Œæœ¯å‰3å¤©ç”¨äººå·¥æ³ªæ¶²æ”¹å–„çœ¼è¡¨ï¼›
3. æ’é™¤æœ¯å‰è§’è†œç—…å˜ã€‚

ã€æœ¯ä¸­å»ºè®®ã€‘
1. æ€»æ‰‹æœ¯æ—¶é—´ç¼©çŸ­è‡³{round(total_surgery_time*0.8)}ç§’å†…ï¼›
2. è´Ÿå‹æ—¶é—´æ§åˆ¶åœ¨{round(negative_pressure_time*0.9)}ç§’ä»¥ä¸‹ï¼›
3. æœ‰æ•ˆä¹³åŒ–æ—¶é—´æ§åˆ¶åœ¨{round(effective_emulsification_time*0.8)}ç§’å†…ã€‚

ã€æœ¯åå»ºè®®ã€‘
1. æ¯æ—¥å¤æŸ¥è§’è†œæ°´è‚¿åŠçœ¼å‹ï¼ŒæŒç»­3å¤©ï¼›
2. æ¿€ç´ æ»´çœ¼æ¶²4æ¬¡/æ—¥ï¼ŒæŒç»­2å‘¨ï¼›
3. åŠ ç”¨è§’è†œä¿æŠ¤å‰‚ï¼Œ1ã€2ã€4å‘¨éšè®¿ã€‚"""
    
    elif risk_level == "é«˜é£é™©":
        return f"""ã€æœ¯å‰å»ºè®®ã€‘
1. è§’è†œæŸä¼¤é£é™©{risk_prob}%ï¼ˆé«˜é£é™©ï¼‰ï¼Œå®Œå–„è§’è†œåšåº¦ã€çœ¼å‹æ£€æŸ¥ï¼›
2. å†…çš®ç»†èƒå¯†åº¦<1800ä¸ª/mmÂ²éœ€æ²Ÿé€šæ‰‹æœ¯æ–¹æ¡ˆï¼›
3. å¹´é¾„{age}å²+å‰æˆ¿æ·±åº¦{anterior_chamber_depth}mmï¼Œé«˜å¹´èµ„åŒ»å¸ˆä¸»åˆ€ã€‚

ã€æœ¯ä¸­å»ºè®®ã€‘
1. æ€»æ‰‹æœ¯æ—¶é—´â‰¤400ç§’ï¼ˆå½“å‰{total_surgery_time}ç§’ï¼Œéœ€ç¼©çŸ­{max(0, total_surgery_time-400)}ç§’ï¼‰ï¼›
2. è´Ÿå‹æ—¶é—´â‰¤150ç§’ï¼Œæœ‰æ•ˆä¹³åŒ–æ—¶é—´â‰¤3ç§’ï¼›
3. ç”¨ç²˜å¼¹å‰‚ä¿æŠ¤è§’è†œå†…çš®ã€‚

ã€æœ¯åå»ºè®®ã€‘
1. ç•™é™¢è§‚å¯Ÿ24å°æ—¶ï¼Œæ¯6å°æ—¶è¯„ä¼°æ°´è‚¿ï¼›
2. æ¿€ç´ æ»´çœ¼æ¶²1æ¬¡/å°æ—¶å†²å‡»3å¤©ï¼ŒåŠ ç”¨é«˜æ¸—ç›æ°´ï¼›
3. 1å‘¨å†…æ¯æ—¥å¤æŸ¥ï¼Œ1ä¸ªæœˆå†…æ¯å‘¨å¤æŸ¥ã€‚"""
    
    else:
        return f"æš‚æ— {risk_level}å¯¹åº”çš„ä¸´åºŠå»ºè®®ï¼ˆé£é™©æ¦‚ç‡ï¼š{risk_prob}%ï¼‰"

# ç»˜åˆ¶SHAPèœ‚ç¾¤å›¾ï¼ˆèœ‚å·¢+å †å æ¡å½¢ç»„åˆå›¾ï¼‰
def plot_shap_beeswarm(X_scaled, feature_names):
    """
    ç»˜åˆ¶SHAP
    èœ‚å·¢å›¾+å †å æ¡å½¢å›¾ç»„åˆå›¾ï¼Œå¹¶è¿”å›base64ç¼–ç 
    :param X_scaled: æ ‡å‡†åŒ–åçš„ç‰¹å¾æ•°æ®
    :param feature_names: ç‰¹å¾åç§°åˆ—è¡¨
    :return: base64ç¼–ç çš„å›¾ç‰‡å­—ç¬¦ä¸²
    """
    global shap_beeswarm_base64, global_shap_values, global_y_values
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“å’Œå›¾ç‰‡æ ·å¼
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial']  # æ”¯æŒä¸­æ–‡
    plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
    plt.figure(figsize=(20, 8))  # è°ƒæ•´å°ºå¯¸é€‚é…ç»„åˆå›¾
    
    try:
        # è®¡ç®—SHAPå€¼ - å…¼å®¹ä¸åŒSHAPç‰ˆæœ¬çš„è¿”å›æ ¼å¼
        shap_values = shap_explainer.shap_values(X_scaled)
        
        # å¤„ç†åˆ†ç±»æ¨¡å‹çš„SHAPå€¼ï¼ˆäºŒåˆ†ç±»æ¨¡å‹è¿”å›listï¼Œå–æ­£ç±»ï¼›å¤šåˆ†ç±»/å›å½’è¿”å›arrayï¼‰
        if isinstance(shap_values, list) and len(shap_values) == 2:
            # äºŒåˆ†ç±»æ¨¡å‹ï¼Œå–æ­£ç±»çš„SHAPå€¼
            shap_values_pos = shap_values[1]
        elif isinstance(shap_values, np.ndarray):
            # å›å½’/å¤šåˆ†ç±»å•è¾“å‡º
            shap_values_pos = shap_values
        else:
            # å…¶ä»–æƒ…å†µå–ç¬¬ä¸€ä¸ªç»´åº¦
            shap_values_pos = shap_values[0] if len(shap_values) > 0 else shap_values
        
        # ä¿å­˜å…¨é‡SHAPå€¼åˆ°å…¨å±€å˜é‡
        global_shap_values = shap_values_pos
        
        # ç­›é€‰TOP Nç‰¹å¾ï¼ˆæŒ‰å¹³å‡ç»å¯¹SHAPå€¼é™åºï¼‰
        feat_importance = np.abs(shap_values_pos).mean(axis=0)
        top_idx = np.argsort(feat_importance)[::-1][:TOP_N_FEATURES]
        top_feat_names = [feature_names[i] for i in top_idx]
        top_shap_values = shap_values_pos[:, top_idx]
        top_feature_values = X_scaled[:, top_idx]
        
        # åˆ›å»ºç»„åˆå›¾ï¼ˆ1è¡Œ2åˆ—ï¼‰
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8), gridspec_kw={'width_ratios': [1, 1]})
        
        # å·¦ä¾§ï¼šèœ‚å·¢å›¾ï¼ˆåŸèœ‚ç¾¤å›¾ï¼‰
        plt.sca(ax1)
        shap.summary_plot(
            top_shap_values,
            features=top_feature_values,
            feature_names=top_feat_names,
            plot_type="dot",
            max_display=TOP_N_FEATURES,
            show=False,
            color_bar_label="ç‰¹å¾å€¼"  # ä¸­æ–‡æ ‡ç­¾
        )
        ax1.tick_params(axis="y", labelsize=9, pad=5)
        ax1.spines['top'].set_visible(False)
        ax1.spines['right'].set_visible(False)
        ax1.set_xlabel("SHAPå€¼", fontsize=12, labelpad=10)  # ä¸­æ–‡æ ‡ç­¾
        
        # å³ä¾§ï¼šå †å æ¡å½¢å›¾ï¼ˆæŒ‰ç±»åˆ«æ‹†åˆ†ï¼‰
        if global_y_values is not None:
            # æŒ‰ç›®æ ‡å€¼åˆ†ç±»è®¡ç®—å¹³å‡ç»å¯¹SHAPå€¼
            y_values = np.array(global_y_values)
            class0_mask = y_values == 0
            class1_mask = y_values == 1
            
            class0_shap = np.abs(top_shap_values[class0_mask]).mean(axis=0) if np.any(class0_mask) else np.zeros(TOP_N_FEATURES)
            class1_shap = np.abs(top_shap_values[class1_mask]).mean(axis=0) if np.any(class1_mask) else np.zeros(TOP_N_FEATURES)
            
            # æŒ‰æ€»é‡è¦æ€§é™åºæ’åº
            total_importance = class0_shap + class1_shap
            sort_idx = np.argsort(total_importance)
            sorted_feats = [top_feat_names[i] for i in sort_idx]
            sorted_class0 = class0_shap[sort_idx]
            sorted_class1 = class1_shap[sort_idx]
            
            # ç»˜åˆ¶å †å æ¡å½¢å›¾
            bar_width = 0.8
            y_pos = np.arange(len(sorted_feats))
            ax2.barh(y_pos, sorted_class0, height=bar_width, color="#4575b4", label="0-2çº§æ°´è‚¿")
            ax2.barh(y_pos, sorted_class1, height=bar_width, left=sorted_class0, color="#d73027", label="3çº§æ°´è‚¿")
            
            # ç¾åŒ–æ¡å½¢å›¾
            ax2.set_yticks(y_pos)
            ax2.set_yticklabels(sorted_feats, fontsize=9)
            ax2.set_xlabel("å¹³å‡ç»å¯¹SHAPå€¼ï¼ˆç‰¹å¾é‡è¦æ€§ï¼‰", fontsize=11)
            ax2.spines['top'].set_visible(False)
            ax2.spines['right'].set_visible(False)
            ax2.legend(
                handles=[Patch(facecolor="#4575b4", label="0-2çº§æ°´è‚¿"), Patch(facecolor="#d73027", label="3çº§æ°´è‚¿")],
                fontsize=8, loc="lower right"
            )
            ax2.set_xlim(0, max(total_importance) * 1.1)
        
        # è°ƒæ•´æ•´ä½“å¸ƒå±€
        plt.tight_layout(rect=[0, 0, 1, 0.95])
        
        # å°†å›¾ç‰‡ä¿å­˜åˆ°BytesIOå¹¶è½¬æ¢ä¸ºbase64
        buf = BytesIO()
        plt.savefig(buf, format='png', dpi=300, bbox_inches='tight')
        buf.seek(0)
        img_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')
        shap_beeswarm_base64 = img_base64
        
        # ä¿å­˜å›¾ç‰‡åˆ°æ–‡ä»¶
        plt.savefig(SHAP_BEESWARM_PATH, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ… SHAPç»„åˆå›¾ï¼ˆèœ‚å·¢+æ¡å½¢ï¼‰å·²ä¿å­˜åˆ°: {os.path.abspath(SHAP_BEESWARM_PATH)}")
        return img_base64
        
    except Exception as e:
        print(f"âš ï¸ ç»˜åˆ¶SHAPç»„åˆå›¾æ—¶å‡ºé”™: {str(e)}")
        # é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨åŸºç¡€çš„summary_plot
        plt.clf()
        shap.summary_plot(
            shap_explainer.shap_values(X_scaled),
            X_scaled,
            feature_names=feature_names,
            show=False,
            max_display=20
        )
        buf = BytesIO()
        plt.savefig(buf, format='png', dpi=300, bbox_inches='tight')
        buf.seek(0)
        img_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')
        shap_beeswarm_base64 = img_base64
        plt.savefig(SHAP_BEESWARM_PATH, dpi=300, bbox_inches='tight')
        plt.close()
        return img_base64

# ç»˜åˆ¶SHAPæ•£ç‚¹æ‹Ÿåˆå›¾
def plot_shap_scatter_fit(X_scaled, feature_names):
    """
    ç»˜åˆ¶SHAPæ•£ç‚¹æ‹Ÿåˆå›¾ï¼ˆå¸¦LOWESSæ‹Ÿåˆæ›²çº¿ï¼‰ï¼Œè¿”å›base64ç¼–ç 
    :param X_scaled: æ ‡å‡†åŒ–åçš„ç‰¹å¾æ•°æ®
    :param feature_names: ç‰¹å¾åç§°åˆ—è¡¨
    :return: base64ç¼–ç çš„å›¾ç‰‡å­—ç¬¦ä¸²
    """
    global shap_scatter_base64, global_shap_values
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“å’Œå›¾ç‰‡æ ·å¼
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial']  # æ”¯æŒä¸­æ–‡
    plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
    plt.rcParams["figure.dpi"] = 300
    plt.rcParams["font.size"] = 10
    
    try:
        # è·å–å…¨å±€SHAPå€¼
        if global_shap_values is None:
            shap_values = shap_explainer.shap_values(X_scaled)
            shap_values_pos = shap_values[1] if isinstance(shap_values, list) and len(shap_values) == 2 else shap_values
            global_shap_values = shap_values_pos
        
        # ç­›é€‰TOP Nç‰¹å¾ï¼ˆæŒ‰å¹³å‡ç»å¯¹SHAPå€¼é™åºï¼‰
        feat_importance = np.abs(global_shap_values).mean(axis=0)
        top_idx = np.argsort(feat_importance)[::-1][:TOP_N_FEATURES]
        top_feat_names = [feature_names[i] for i in top_idx]
        top_shap_values = global_shap_values[:, top_idx]
        top_feature_values = X_scaled[:, top_idx]
        
        # é€‰å–å‰12ä¸ªç‰¹å¾ç»˜åˆ¶
        plot_feats = top_feat_names[:12]
        plot_shap = top_shap_values[:, :12]
        plot_features = top_feature_values[:, :12]
        
        # åˆ›å»ºç”»å¸ƒ
        fig = plt.figure(figsize=(12, 12))
        gs = GridSpec(4, 3, figure=fig, wspace=0.5, hspace=0.3)
        lowess_color = '#457B9D'  # LOWESSæ›²çº¿é¢œè‰²
        cmap = plt.cm.coolwarm    # æ•£ç‚¹é¢œè‰²æ˜ å°„

        # å¾ªç¯ç»˜åˆ¶æ¯ä¸ªç‰¹å¾
        for i, feat in enumerate(plot_feats):
            # å­ç½‘æ ¼ï¼ˆæ•£ç‚¹å›¾+é¢œè‰²æ£’ï¼‰
            sub_gs = gs[i//3, i%3].subgridspec(1, 2, width_ratios=[30, 1], wspace=0.05)
            ax_scatter = plt.subplot(sub_gs[0, 0])
            
            # æå–æ•°æ®
            x = plot_features[:, i]
            y = plot_shap[:, i]
            
            # é¢œè‰²æ˜ å°„
            norm = plt.Normalize(vmin=np.min(x), vmax=np.max(x))
            scatter = ax_scatter.scatter(
                x, y, alpha=0.7, s=10, c=x, cmap=cmap, norm=norm,
                edgecolor='k', linewidth=0.6, zorder=3
            )
            
            # LOWESSæ‹Ÿåˆ
            try:
                lowess_fit = lowess(y, x, frac=0.35)
                ax_scatter.plot(lowess_fit[:, 0], lowess_fit[:, 1], 
                               color=lowess_color, linewidth=1.8, alpha=0.9, zorder=4)
            except Exception as e:
                print(f"ç‰¹å¾ {feat} LOWESSæ‹Ÿåˆå¤±è´¥: {e}")
            
            # y=0çº¢çº¿
            ax_scatter.axhline(y=0, color='#E63946', linestyle='--', linewidth=1, alpha=0.8, zorder=2)
            
            # ç¾åŒ–
            ax_scatter.set_xlabel(feat, fontsize=12)
            ax_scatter.set_ylabel('SHAP Value', fontsize=12)
            ax_scatter.spines['top'].set_visible(False)
            ax_scatter.spines['right'].set_visible(False)
            ax_scatter.spines['left'].set_linewidth(0.9)
            ax_scatter.spines['bottom'].set_linewidth(0.9)
            
            # é¢œè‰²æ£’
            ax_colorbar = plt.subplot(sub_gs[0, 1])
            cbar = fig.colorbar(scatter, cax=ax_colorbar, orientation='vertical')
            cbar.ax.tick_params(labelsize=10)
            cbar.set_label('Feature Value', fontsize=12, labelpad=3)
            
            # è°ƒæ•´é¢œè‰²æ£’åˆ»åº¦
            if np.max(x) - np.min(x) > 5:
                cbar.locator = plt.MaxNLocator(nbins=3)
                cbar.update_ticks()

        # è°ƒæ•´å¸ƒå±€å¹¶ä¿å­˜
        plt.subplots_adjust(left=0.05, right=0.98, hspace=0.3, wspace=0.5)
        
        # å°†å›¾ç‰‡ä¿å­˜åˆ°BytesIOå¹¶è½¬æ¢ä¸ºbase64
        buf = BytesIO()
        plt.savefig(buf, format='png', dpi=300, bbox_inches='tight')
        buf.seek(0)
        img_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')
        shap_scatter_base64 = img_base64
        
        # ä¿å­˜å›¾ç‰‡åˆ°æ–‡ä»¶
        plt.savefig(SHAP_SCATTER_PATH, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ… SHAPæ•£ç‚¹æ‹Ÿåˆå›¾å·²ä¿å­˜åˆ°: {os.path.abspath(SHAP_SCATTER_PATH)}")
        return img_base64
        
    except Exception as e:
        print(f"âš ï¸ ç»˜åˆ¶SHAPæ•£ç‚¹æ‹Ÿåˆå›¾æ—¶å‡ºé”™: {str(e)}")
        plt.close()
        raise e

# æ¨¡å‹è¯„ä¼°ä¸ROCæ›²çº¿
def evaluate_model(X_train_scaled, y_train, X_test_scaled, y_test):
    y_train_prob = model.predict_proba(X_train_scaled)[:, 1].tolist()
    y_test_prob = model.predict_proba(X_test_scaled)[:, 1].tolist()
    y_train_pred = (np.array(y_train_prob) >= 0.5).astype(int).tolist()
    y_test_pred = (np.array(y_test_prob) >= 0.5).astype(int).tolist()

    metrics = {
        "Training Set": {
            "AUC": round(roc_auc_score(y_train, y_train_prob), 4),
            "Accuracy": round(accuracy_score(y_train, y_train_pred), 4),
            "Precision": round(precision_score(y_train, y_train_pred, zero_division=0), 4),
            "Recall": round(recall_score(y_train, y_train_pred, zero_division=0), 4),
            "F1-Score": round(f1_score(y_train, y_train_pred, zero_division=0), 4),
            "Confusion Matrix": confusion_matrix(y_train, y_train_pred).tolist()
        },
        "Test Set": {
            "AUC": round(roc_auc_score(y_test, y_test_prob), 4),
            "Accuracy": round(accuracy_score(y_test, y_test_pred), 4),
            "Precision": round(precision_score(y_test, y_test_pred, zero_division=0), 4),
            "Recall": round(recall_score(y_test, y_test_pred, zero_division=0), 4),
            "F1-Score": round(f1_score(y_test, y_test_pred, zero_division=0), 4),
            "Confusion Matrix": confusion_matrix(y_test, y_test_pred).tolist()
        }
    }

    print("\n" + "="*80)
    print("ğŸ“Š Model Evaluation Results")
    print("="*80)
    for set_name, set_metrics in metrics.items():
        print(f"\nã€{set_name}ã€‘")
        for k, v in set_metrics.items():
            if k != "Confusion Matrix":
                print(f"  {k:<12} : {v}")
            else:
                print(f"  {k}:")
                for row in v:
                    print(f"    {row}")
    print("="*80 + "\n")

    # ç»˜åˆ¶ROCæ›²çº¿
    plt.rcParams['font.sans-serif'] = ['Arial', 'SimHei']
    plt.figure(figsize=(8, 6))
    fpr_train, tpr_train, _ = roc_curve(y_train, y_train_prob)
    fpr_test, tpr_test, _ = roc_curve(y_test, y_test_prob)
    plt.plot(fpr_train, tpr_train, label=f"Training Set (AUC = {metrics['Training Set']['AUC']})", 
             linewidth=2.5, color="#2E86AB")
    plt.plot(fpr_test, tpr_test, label=f"Test Set (AUC = {metrics['Test Set']['AUC']})", 
             linewidth=2.5, color="#A23B72")
    plt.plot([0, 1], [0, 1], "k--", label="Random Guess", alpha=0.7)
    plt.xlabel("False Positive Rate (FPR)", fontsize=14, fontweight="bold")
    plt.ylabel("True Positive Rate (TPR)", fontsize=14, fontweight="bold")
    plt.legend(fontsize=12, loc="lower right")
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig(ROC_SAVE_PATH, dpi=300, bbox_inches="tight")
    print(f"âœ… ROC saved to: {os.path.abspath(ROC_SAVE_PATH)}")
    plt.close()

# æ•°æ®å¤„ç†ä¸æ¨¡å‹è®­ç»ƒ
def load_data_and_train():
    global model, scaler, shap_explainer, FEATURE_NAMES, shap_beeswarm_base64, shap_scatter_base64
    global global_shap_values, global_feature_values, global_y_values

    # è¯»å–Excel
    if not os.path.exists(EXCEL_PATH):
        raise FileNotFoundError(f"Excel not found: {EXCEL_PATH}")
    df = pd.read_excel(EXCEL_PATH)
    print(f"âœ… Excel loaded: {df.shape[0]} rows, {df.shape[1]} columns")

    # æ ¡éªŒç›®æ ‡åˆ—
    if TARGET_COL not in df.columns:
        raise HTTPException(status_code=500, detail=f"Target column '{TARGET_COL}' not found")

    # ç‰¹å¾åˆ—è¯†åˆ«
    FEATURE_NAMES = [col for col in df.columns if col != TARGET_COL]
    if len(FEATURE_NAMES) < 1:
        raise HTTPException(status_code=500, detail="No feature columns found")
    print(f"âœ… Features (ä¸­æ–‡åˆ—å): {FEATURE_NAMES}")

    # ç¼ºå¤±å€¼å¤„ç†
    df_clean = df.dropna(subset=FEATURE_NAMES + [TARGET_COL])
    if df_clean.shape[0] < 10:
        raise HTTPException(status_code=500, detail="Too few data rows after cleaning")

    # æ•°æ®å‡†å¤‡
    X = df_clean[FEATURE_NAMES].values
    y = np.where(df_clean[TARGET_COL].values != 0, 1, 0).tolist()
    
    # ä¿å­˜å…¨é‡åŸå§‹ç‰¹å¾å€¼å’Œç›®æ ‡å€¼åˆ°å…¨å±€å˜é‡
    global_feature_values = X
    global_y_values = y  # ä¿å­˜ç›®æ ‡å€¼

    # åˆ’åˆ†æ•°æ®é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )

    # SMOTEè¿‡é‡‡æ ·
    X_train_smote, y_train_smote = X_train, y_train
    if y_train.count(1) < y_train.count(0) and y_train.count(1) >= 2:
        scaler_temp = StandardScaler()
        X_train_scaled_temp = scaler_temp.fit_transform(X_train)
        smote = SMOTE(random_state=42, k_neighbors=min(5, y_train.count(1)-1))
        X_train_smote_scaled, y_train_smote = smote.fit_resample(X_train_scaled_temp, y_train)
        X_train_smote = scaler_temp.inverse_transform(X_train_smote_scaled)
        print(f"âœ… SMOTE applied: {X_train_smote.shape}")

    # ä¿å­˜æ•°æ®é›†åˆ°Excel
    with pd.ExcelWriter(DATA_SAVE_EXCEL_PATH, engine='openpyxl') as writer:
        train_df = pd.DataFrame(X_train, columns=FEATURE_NAMES)
        train_df[TARGET_COL] = y_train
        train_df.to_excel(writer, sheet_name='è®­ç»ƒé›†_åŸå§‹', index=False)
        
        test_df = pd.DataFrame(X_test, columns=FEATURE_NAMES)
        test_df[TARGET_COL] = y_test
        test_df.to_excel(writer, sheet_name='æµ‹è¯•é›†', index=False)
        
        smote_train_df = pd.DataFrame(X_train_smote, columns=FEATURE_NAMES)
        smote_train_df[TARGET_COL] = y_train_smote
        smote_train_df.to_excel(writer, sheet_name='è®­ç»ƒé›†_SMOTEå', index=False)
    
    print(f"âœ… æ•°æ®é›†å·²ä¿å­˜åˆ°Excel: {os.path.abspath(DATA_SAVE_EXCEL_PATH)}")
    print(f"   - å·¥ä½œè¡¨1: è®­ç»ƒé›†_åŸå§‹ (è¡Œæ•°: {len(X_train)})")
    print(f"   - å·¥ä½œè¡¨2: æµ‹è¯•é›† (è¡Œæ•°: {len(X_test)})")
    print(f"   - å·¥ä½œè¡¨3: è®­ç»ƒé›†_SMOTEå (è¡Œæ•°: {len(X_train_smote)})")

    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    joblib.dump(scaler, SCALER_PATH)

    # é‡æ–°æ ‡å‡†åŒ–SMOTEåçš„è®­ç»ƒé›†
    X_train_smote_scaled = scaler.transform(X_train_smote)

    # è®­ç»ƒæ¨¡å‹
    model = GradientBoostingClassifier(
        n_estimators=450,
        learning_rate=0.0529678687405566,
        max_depth=7,
        min_samples_split=5,
        min_samples_leaf=2,
        subsample=0.6251214490822976,
        max_features='log2',
        loss='exponential',
        random_state=42
    )
    model.fit(X_train_smote_scaled, y_train_smote)
    joblib.dump({"model": model, "feature_names": FEATURE_NAMES}, MODEL_PATH)
    print(f"âœ… Model saved to {MODEL_PATH}")

    # è¯„ä¼°æ¨¡å‹
    evaluate_model(X_train_scaled, y_train, X_test_scaled, y_test)
    
    # åˆå§‹åŒ–SHAPè§£é‡Šå™¨
    shap_explainer = shap.TreeExplainer(model)
    joblib.dump(shap_explainer, SHAP_EXPLAINER_PATH)
    
    # æ ‡å‡†åŒ–æ•´ä¸ªæ•°æ®é›†ç”¨äºSHAPè®¡ç®—
    X_full_scaled = scaler.transform(X)
    
    # ç»˜åˆ¶æ•´ä¸ªæ•°æ®é›†çš„SHAPèœ‚ç¾¤å›¾ï¼ˆç°åœ¨æ˜¯ç»„åˆå›¾ï¼‰
    plot_shap_beeswarm(X_full_scaled, FEATURE_NAMES)
    
    # ç»˜åˆ¶æ•´ä¸ªæ•°æ®é›†çš„SHAPæ•£ç‚¹æ‹Ÿåˆå›¾
    plot_shap_scatter_fit(X_full_scaled, FEATURE_NAMES)
    
    print(f"âœ… SHAP explainer saved")

# æœåŠ¡åˆå§‹åŒ–
try:
    if os.path.exists(MODEL_PATH) and os.path.exists(SCALER_PATH) and os.path.exists(SHAP_EXPLAINER_PATH):
        model_data = joblib.load(MODEL_PATH)
        model = model_data["model"]
        FEATURE_NAMES = model_data["feature_names"]
        scaler = joblib.load(SCALER_PATH)
        shap_explainer = joblib.load(SHAP_EXPLAINER_PATH)
        
        # åŠ è½½å…¨é‡æ•°æ®å¹¶è®¡ç®—å…¨å±€SHAPå€¼
        if os.path.exists(EXCEL_PATH):
            df = pd.read_excel(EXCEL_PATH)
            df_clean = df.dropna(subset=FEATURE_NAMES + [TARGET_COL])
            X = df_clean[FEATURE_NAMES].values
            y = np.where(df_clean[TARGET_COL].values != 0, 1, 0).tolist()
            global_feature_values = X  # ä¿å­˜å…¨é‡åŸå§‹ç‰¹å¾å€¼
            global_y_values = y        # ä¿å­˜ç›®æ ‡å€¼
            X_full_scaled = scaler.transform(X)
            
            # è®¡ç®—å¹¶ä¿å­˜å…¨å±€SHAPå€¼
            shap_values = shap_explainer.shap_values(X_full_scaled)
            if isinstance(shap_values, list) and len(shap_values) == 2:
                global_shap_values = shap_values[1]  # äºŒåˆ†ç±»å–æ­£ç±»
            else:
                global_shap_values = shap_values
            
            # å¦‚æœæœªç”ŸæˆSHAPèœ‚ç¾¤å›¾åˆ™ç”Ÿæˆ
            if not os.path.exists(SHAP_BEESWARM_PATH):
                plot_shap_beeswarm(X_full_scaled, FEATURE_NAMES)
            
            # å¦‚æœæœªç”ŸæˆSHAPæ•£ç‚¹æ‹Ÿåˆå›¾åˆ™ç”Ÿæˆ
            if not os.path.exists(SHAP_SCATTER_PATH):
                plot_shap_scatter_fit(X_full_scaled, FEATURE_NAMES)
        
        print(f"\nâœ… Model loaded. Features (ä¸­æ–‡): {FEATURE_NAMES}")
        print(f"âœ… Global SHAP values loaded: {global_shap_values.shape if global_shap_values is not None else 'None'}")
    else:
        print(f"\nâš ï¸ No model found. Training...")
        load_data_and_train()
    print(f"\nğŸš€ Service ready: http://localhost:8000/docs")
except Exception as e:
    print(f"âŒ Init failed: {str(e)}")
    # æ‰“å°è¯¦ç»†çš„é”™è¯¯å †æ ˆä¿¡æ¯
    import traceback
    traceback.print_exc()
    raise HTTPException(status_code=500, detail=f"Init failed: {str(e)}")

# é¢„æµ‹æ¥å£
@app.post("/predict")
async def predict_corneal_damage(
    params: dict = Body(..., example={
        "å¹´é¾„": 60.0,
        "æœ€ä½³çŸ«æ­£è§†åŠ›": 0.5,
        "å‰æˆ¿æ·±åº¦": 3.0,
        "å‰æˆ¿å®¹ç§¯": 150.0,
        "æ€»æ‰‹æœ¯æ—¶é—´": 600.0,
        "è´Ÿå‹æ—¶é—´": 200.0,
        "æœ‰æ•ˆä¹³åŒ–æ—¶é—´": 5.0
    })
):
    try:
        # æ ¡éªŒå‚æ•°å®Œæ•´æ€§
        missing_features = [col for col in FEATURE_NAMES if col not in params]
        if missing_features:
            raise ValueError(f"Missing parameters (ç¼ºå¤±å‚æ•°): {missing_features}")

        # æ ¡éªŒå‚æ•°ç±»å‹
        input_list = []
        for col in FEATURE_NAMES:
            val = params[col]
            if not isinstance(val, (int, float)):
                raise ValueError(f"Parameter '{col}' must be number, got {type(val)}")
            input_list.append(float(val))

        # æ ‡å‡†åŒ–ä¸é¢„æµ‹
        input_data = np.array(input_list).reshape(1, -1)
        input_scaled = scaler.transform(input_data)
        risk_prob = model.predict_proba(input_scaled)[0][1]
        risk_prob = float(round(risk_prob, 4))

        # é£é™©ç­‰çº§
        if risk_prob > 0.7:
            risk_level = "é«˜é£é™©"
        elif risk_prob > 0.4:
            risk_level = "ä¸­é£é™©"
        else:
            risk_level = "ä½é£é™©"

        # å•ä¸ªæ ·æœ¬çš„SHAPå€¼
        shap_values = [0.0 for _ in FEATURE_NAMES]
        if shap_explainer:
            try:
                shap_result = shap_explainer.shap_values(input_scaled)
                shap_array = shap_result[1] if isinstance(shap_result, list) else shap_result
                shap_list = shap_array[0].tolist()
                shap_values = [float(round(val, 4)) for val in shap_list]
            except Exception as e:
                print(f"âš ï¸ SHAP failed: {str(e)}")
                shap_values = [0.0 for _ in FEATURE_NAMES]

        # ç½®ä¿¡åº¦
        confidence = float(round(0.85 + (min(risk_prob, 1 - risk_prob) * 0.13), 4))
        
        # ç”Ÿæˆä¸´åºŠå»ºè®®ï¼ˆé£é™©æ¦‚ç‡ä¿ç•™2ä½å°æ•°ï¼‰
        risk_prob_percent = round(risk_prob * 100, 2)  # ä¿ç•™2ä½å°æ•°
        clinical_advice = generate_clinical_advice(risk_level, risk_prob_percent, params)

        # å‡†å¤‡å…¨å±€SHAPæ•°æ®ï¼ˆç”¨äºå‰ç«¯ç»˜åˆ¶èœ‚å·¢å›¾ï¼‰
        global_shap_data = []
        global_feature_data = []
        if global_shap_values is not None and global_feature_values is not None:
            global_shap_data = global_shap_values.tolist()
            global_feature_data = global_feature_values.tolist()

        # è¿”å›ç»“æœï¼ˆå…¨å±€SHAPæ•°æ®ï¼‰
        return {
            "code": 200,
            "message": "Prediction Successful",
            "data": {
                "feature_names": FEATURE_NAMES,
                "risk_probability": risk_prob,
                "confidence": confidence,
                "risk_level": risk_level,
                "shap_values": shap_values,          # å•ä¸ªæ ·æœ¬çš„SHAPå€¼
                "global_shap_values": global_shap_data,  # å…¨é‡æ ·æœ¬çš„SHAPå€¼
                "global_feature_values": global_feature_data,  # å…¨é‡æ ·æœ¬çš„åŸå§‹ç‰¹å¾å€¼
                "clinical_advice": clinical_advice
            }
        }
    except Exception as e:
        error_msg = f"Prediction failed: {str(e)}"
        print(f"âŒ {error_msg}")
        return {"code": 500, "message": error_msg, "data": None}

# å¥åº·æ£€æŸ¥æ¥å£
@app.get("/health")
async def health_check():
    return {
        "code": 200,
        "message": "Service Running Normally",
        "data": {
            "model_loaded": bool(model),
            "feature_names": FEATURE_NAMES,
            "shap_supported": bool(shap_explainer),
            "required_params": FEATURE_NAMES,
            "param_count": len(FEATURE_NAMES),
            "shap_beeswarm_generated": os.path.exists(SHAP_BEESWARM_PATH),
            "shap_scatter_generated": os.path.exists(SHAP_SCATTER_PATH),
            "global_shap_available": global_shap_values is not None
        }
    }

# è·å–SHAPèœ‚ç¾¤å›¾æ¥å£
@app.get("/get_shap_beeswarm")
async def get_shap_beeswarm():
    """è¿”å›SHAPç»„åˆå›¾çš„base64ç¼–ç ï¼Œç”¨äºå‰ç«¯åµŒå…¥"""
    try:
        global shap_beeswarm_base64
        
        # å¦‚æœbase64æœªç¼“å­˜ï¼Œä»æ–‡ä»¶è¯»å–å¹¶è½¬æ¢
        if not shap_beeswarm_base64:
            if not os.path.exists(SHAP_BEESWARM_PATH):
                # é‡æ–°ç”Ÿæˆå›¾ç‰‡
                df = pd.read_excel(EXCEL_PATH)
                df_clean = df.dropna(subset=FEATURE_NAMES + [TARGET_COL])
                X = df_clean[FEATURE_NAMES].values
                X_full_scaled = scaler.transform(X)
                plot_shap_beeswarm(X_full_scaled, FEATURE_NAMES)
            
            with open(SHAP_BEESWARM_PATH, 'rb') as f:
                img_base64 = base64.b64encode(f.read()).decode('utf-8')
                shap_beeswarm_base64 = img_base64
        
        return {
            "code": 200,
            "message": "SHAP beeswarm plot retrieved successfully",
            "data": {
                "image_base64": shap_beeswarm_base64,
                "image_type": "png"
            }
        }
    except Exception as e:
        error_msg = f"Failed to get SHAP beeswarm plot: {str(e)}"
        print(f"âŒ {error_msg}")
        # æ‰“å°è¯¦ç»†é”™è¯¯
        import traceback
        traceback.print_exc()
        return {"code": 500, "message": error_msg, "data": None}

# è·å–SHAPæ•£ç‚¹æ‹Ÿåˆå›¾æ¥å£
@app.get("/get_shap_scatter")
async def get_shap_scatter():
    """è¿”å›SHAPæ•£ç‚¹æ‹Ÿåˆå›¾çš„base64ç¼–ç ï¼Œç”¨äºå‰ç«¯åµŒå…¥"""
    try:
        global shap_scatter_base64
        
        # å¦‚æœbase64æœªç¼“å­˜ï¼Œä»æ–‡ä»¶è¯»å–å¹¶è½¬æ¢
        if not shap_scatter_base64:
            if not os.path.exists(SHAP_SCATTER_PATH):
                # é‡æ–°ç”Ÿæˆå›¾ç‰‡
                df = pd.read_excel(EXCEL_PATH)
                df_clean = df.dropna(subset=FEATURE_NAMES + [TARGET_COL])
                X = df_clean[FEATURE_NAMES].values
                X_full_scaled = scaler.transform(X)
                plot_shap_scatter_fit(X_full_scaled, FEATURE_NAMES)
            
            with open(SHAP_SCATTER_PATH, 'rb') as f:
                img_base64 = base64.b64encode(f.read()).decode('utf-8')
                shap_scatter_base64 = img_base64
        
        return {
            "code": 200,
            "message": "SHAP scatter plot retrieved successfully",
            "data": {
                "image_base64": shap_scatter_base64,
                "image_type": "png"
            }
        }
    except Exception as e:
        error_msg = f"Failed to get SHAP scatter plot: {str(e)}"
        print(f"âŒ {error_msg}")
        # æ‰“å°è¯¦ç»†é”™è¯¯
        import traceback
        traceback.print_exc()
        return {"code": 500, "message": error_msg, "data": None}

# æ¨¡å‹ä¸Šä¼ æ¥å£
@app.post("/upload_model")
async def upload_model(file: UploadFile = File(...)):
    global model, FEATURE_NAMES, shap_explainer, shap_beeswarm_base64, shap_scatter_base64
    global global_shap_values, global_feature_values, global_y_values
    try:
        if not file.filename.endswith((".pkl", ".joblib")):
            raise ValueError("Only .pkl/.joblib files are allowed")

        # ä¿å­˜æ¨¡å‹
        with open(MODEL_PATH, "wb") as f:
            f.write(await file.read())

        # åŠ è½½æ–°æ¨¡å‹
        model_data = joblib.load(MODEL_PATH)
        if "model" not in model_data or "feature_names" not in model_data:
            raise ValueError("Model file missing 'model' or 'feature_names'")
        
        model = model_data["model"]
        FEATURE_NAMES = model_data["feature_names"]
        shap_explainer = shap.TreeExplainer(model)
        joblib.dump(shap_explainer, SHAP_EXPLAINER_PATH)
        
        # é‡æ–°è®¡ç®—å…¨å±€SHAPå€¼
        if os.path.exists(EXCEL_PATH):
            df = pd.read_excel(EXCEL_PATH)
            df_clean = df.dropna(subset=FEATURE_NAMES + [TARGET_COL])
            X = df_clean[FEATURE_NAMES].values
            y = np.where(df_clean[TARGET_COL].values != 0, 1, 0).tolist()
            global_feature_values = X
            global_y_values = y  # ä¿å­˜ç›®æ ‡å€¼
            X_full_scaled = scaler.transform(X)
            
            # è®¡ç®—å…¨å±€SHAPå€¼
            shap_values = shap_explainer.shap_values(X_full_scaled)
            if isinstance(shap_values, list) and len(shap_values) == 2:
                global_shap_values = shap_values[1]
            else:
                global_shap_values = shap_values
            
            # é‡æ–°ç”ŸæˆSHAPç»„åˆå›¾
            plot_shap_beeswarm(X_full_scaled, FEATURE_NAMES)
            
            # é‡æ–°ç”ŸæˆSHAPæ•£ç‚¹æ‹Ÿåˆå›¾
            plot_shap_scatter_fit(X_full_scaled, FEATURE_NAMES)

        print(f"âœ… Custom model loaded. Features (ä¸­æ–‡): {FEATURE_NAMES}")
        return {
            "code": 200,
            "message": "Model Uploaded Successfully",
            "data": {"feature_names": FEATURE_NAMES}
        }
    except Exception as e:
        error_msg = f"Model upload failed: {str(e)}"
        print(f"âŒ {error_msg}")
        # æ‰“å°è¯¦ç»†é”™è¯¯
        import traceback
        traceback.print_exc()
        return {"code": 500, "message": error_msg, "data": None}

# å¯åŠ¨æœåŠ¡
if __name__ == "__main__":
    import uvicorn
    print(f"\nğŸ“Œ Starting server: http://localhost:8000")
    uvicorn.run(
        __file__.replace("\\", "/").split("/")[-1].split(".")[0] + ":app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )